[![Version](https://img.shields.io/pypi/v/openpredict)](https://pypi.org/project/openpredict) [![Python versions](https://img.shields.io/pypi/pyversions/openpredict)](https://pypi.org/project/openpredict) [![Run tests](https://github.com/MaastrichtU-IDS/translator-openpredict/workflows/Run%20tests/badge.svg)](https://github.com/MaastrichtU-IDS/translator-openpredict/actions?query=workflow%3A%22Run+tests%22) [![Publish package](https://github.com/MaastrichtU-IDS/translator-openpredict/workflows/Publish%20package/badge.svg)](https://github.com/MaastrichtU-IDS/translator-openpredict/actions?query=workflow%3A%22Publish+package%22) [![SonarCloud Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=MaastrichtU-IDS_translator-openpredict&metric=alert_status)](https://sonarcloud.io/dashboard?id=MaastrichtU-IDS_translator-openpredict) [![SonarCloud Maintainability Rating](https://sonarcloud.io/api/project_badges/measure?project=MaastrichtU-IDS_translator-openpredict&metric=sqale_rating)](https://sonarcloud.io/dashboard?id=MaastrichtU-IDS_translator-openpredict) [![SonarCloud Coverage](https://sonarcloud.io/api/project_badges/measure?project=MaastrichtU-IDS_translator-openpredict&metric=coverage)](https://sonarcloud.io/dashboard?id=MaastrichtU-IDS_translator-openpredict)

Documentation to deploy the **Translator OpenPredict API**.

> Contributions, [feedbacks](https://github.com/MaastrichtU-IDS/translator-openpredict/issues) and pull requests are welcomed!

This repository uses [GitHub Actions](https://github.com/MaastrichtU-IDS/translator-openpredict/actions) to:

* Automatically run tests at each push to the `master` branch
* Publish the [OpenPredict package to PyPI](https://pypi.org/project/openpredict/) when a release is created (N.B.: the version of the package needs to be increased in [setup.py](https://github.com/MaastrichtU-IDS/translator-openpredict/blob/master/setup.py#L6) before).

See [here](https://maastrichtu-ids.github.io/translator-openpredict/docs) to browse the Python code documentation automatically generated by [pydoc-markdown](https://pydoc-markdown.readthedocs.io/en/latest/) üìñ

# Install OpenPredict üì•

> Requires [Python 3.6+](https://www.python.org/downloads/) and [pip](https://pip.pypa.io/en/stable/installing/)

Install `openpredict` locally, if you want to run **OpenPredict** in development, make changes to the source code, and build new models.

The **OpenPredict API store its data in a  RDF triplestore**. We use [Ontotext GraphDB](https://github.com/Ontotext-AD/graphdb-docker) at IDS, but you are free to use any other triplestore. You can pass the credentials using environment variables `SPARQL_USER` and `SPARQL_PASSWORD`

### Install from PyPI

Install the latest release published on [PyPI üè∑Ô∏è](https://pypi.org/project/openpredict) (or see below to [run the API with Docker](#option-3-run-with-docker))

```bash
pip3 install openpredict
```

### Install from the source code

All contribution to OpenPredict are welcome!

Clone this repository:

```bash
git clone https://github.com/MaastrichtU-IDS/translator-openpredict.git
cd translator-openpredict
```

This will install `openpredict` and update the package automatically when the files changes locally üîÉ

```bash
pip3 install -e .
```

#### Optional: isolate with a Virtual Environment

If you are facing conflict with already installed packages, then you might want to use a [Virtual Environment](https://docs.python.org/3/tutorial/venv.html) to isolate the installation in the current folder before installing OpenPredict:

```bash
# Create the virtual environment folder in your workspace
python3 -m venv .venv
# Activate it using a script in the created folder
source .venv/bin/activate
```

# Deploy OpenPredict API

The OpenPredict API store its data using a  RDF triplestore. We use [Ontotext GraphDB](https://github.com/Ontotext-AD/graphdb-docker) at IDS, but you are free to use any other triplestore.

### Define environment variables

Define the environment variable for the triplestore password in your local terminal:

```bash
export SPARQL_PASSWORD=password
```

Add it to your `~/.bashrc` or `~/.zshrc` file to define it permanently.

> You can also use a different repository and user by setting those environment variables:
>
> ```bash
> export SPARQL_ENDPOINT_URL=https://graphdb.dumontierlab.com/repositories/translator-openpredict-dev
> export SPARQL_ENDPOINT_UPDATE_URL=https://graphdb.dumontierlab.com/repositories/translator-openpredict-dev/statements
> export SPARQL_USER=import_user
> export OPENPREDICT_APIKEY=myapikey
> export OPENPREDICT_DATA_DIR=/data/openpredict
> ```

You can provide the path you want to be used as directory to store models and features files. By default it will do it in a `data` folder in the directory where you started openpredict api.

3 options are available to deploy the API:

### Option 1: Run from the command line ‚å®Ô∏è

Use the `openpredict` CLI to run in development with [Flask üß™](https://flask.palletsprojects.com/en/1.1.x/). The API will reload automatically at each change üîÉ

```bash
openpredict start-api --debug
```

You can also start the API in production settings using [Tornado üå™Ô∏è](https://www.tornadoweb.org/en/stable/)

```bash
openpredict start-api
```

> Access the Swagger UI at [http://localhost:8808](http://localhost:8808)

You can provide the API port as argument:

```bash
openpredict start-api --port 8808
```

### Option 2: Run from a Python script üêç

```python
from openpredict import openpredict_api

openpredict_api.start_api(8808)
```

> Access the Swagger UI at [http://localhost:8808](http://localhost:8808)

> Run by default in production, set `debug = True` to run in development environments. 

### Option 3: Run with Docker üê≥

Running using Docker can be convenient if you just want to run the API without installing the package locally, or if it runs in production alongside other services.

Clone the [repository](https://github.com/MaastrichtU-IDS/translator-openpredict):

```bash
git clone https://github.com/MaastrichtU-IDS/translator-openpredict.git
cd translator-openpredict
```

2. Define the triplestore credentials and API key in the `.env` file üîë

```bash
SPARQL_ENDPOINT_URL=https://graphdb.dumontierlab.com/repositories/translator-openpredict-dev
SPARQL_ENDPOINT_UPDATE_URL=https://graphdb.dumontierlab.com/repositories/translator-openpredict-dev/statements
SPARQL_USER=import_user
SPARQL_PASSWORD=password
OPENPREDICT_APIKEY=apikey
OPENPREDICT_DATA_DIR=/data/openpredict
```

You can provide the path you want to be used as directory to store models and features files. By default it will do it in a `data` folder in the directory where you started openpredict api

* Build and start the **OpenPredict API in Docker** with a Virtuoso triplestore locally using [docker-compose](https://docs.docker.com/compose/)

```bash
docker-compose up -d
```

> Access the Swagger UI at [http://localhost:8808](http://localhost:8808)

> We use [nginx-proxy](https://github.com/nginx-proxy/nginx-proxy) and [docker-letsencrypt-nginx-proxy-companion](https://github.com/nginx-proxy/docker-letsencrypt-nginx-proxy-companion) as reverse proxy for HTTP and HTTPS in production. You can change the proxy URL and port via environment variables `VIRTUAL_HOST`, `VIRTUAL_PORT` and `LETSENCRYPT_HOST` in the [docker-compose.yml](https://github.com/MaastrichtU-IDS/translator-openpredict/blob/master/docker-compose.yml) file.

Check the logs:

```bash
docker-compose logs
```

Stop the container:

```bash
docker-compose down
```

* Or start just the virtuoso triplestore to **develop locally**:

Set the environment variable for Virtuoso in your terminal:

```bash
export SPARQL_USER: dba
export SPARQL_PASSWORD: dba
export SPARQL_ENDPOINT_URL: http://localhost:8890/sparql
export SPARQL_ENDPOINT_UPDATE_URL: http://localhost:8890/sparql
```

Start Virtuoso with docker and the OpenPredict API in debug mode:

```bash
docker-compose up -f docker-compose.dev.yml up -d
# Wait a minute for Virtuoso to start, and start openpredict locally
openpredict start-api --debug
```

* For **production deployment** on [openpredict.semanticscience.org](https://openpredict.semanticscience.org/) use the `docker-compose.prod.yml`

Define the triplestore credentials and API key in the `.env` file üîë

```bash
nano .env
SPARQL_USER=import_user
SPARQL_PASSWORD=password
OPENPREDICT_APIKEY=apikey
```

Start the API in production using GraphDB as backend:

```bash
docker-compose up -f docker-compose.prod.yml up -d
```

Build and push to the [GitHub Docker Container Registry üì¶](https://github.com/orgs/MaastrichtU-IDS/packages/container/package/openpredict-api)

```bash
docker build -t ghcr.io/maastrichtu-ids/openpredict-api .
docker push ghcr.io/maastrichtu-ids/openpredict-api
```

# Run tests ‚úîÔ∏è

[![Run tests](https://github.com/MaastrichtU-IDS/translator-openpredict/workflows/Run%20tests/badge.svg)](https://github.com/MaastrichtU-IDS/translator-openpredict/actions?query=workflow%3A%22Run+tests%22)

Tests are automatically run by a [GitHub Action](https://github.com/MaastrichtU-IDS/translator-openpredict/actions?query=workflow%3A%22Run+tests%22) at each push to the `master` branch. They are also run in the GitHub Action to publish a package.

Run the **OpenPredict API** tests locally:

```bash
pytest tests
```

Run a specific test in a file, and display `print` in the output:

```bash
pytest tests/test_openpredict_api.py::test_post_reasoner_predict -s
```

# Create a new API call üìù

Guidelines to create a new API  call in the OpenPredict Open API.

1. Create the operations in the [openpredict/openapi.yml](https://github.com/MaastrichtU-IDS/translator-openpredict/blob/master/openpredict/openapi.yml#L44) file

Provide the path to the function that will resolve this API call:

```yaml
paths:
  /predict:
    get:
      operationId: openpredict.openpredict_api.get_predict
      parameters:
      - name: entity
        in: query
        description: CURIE of the entity to process (e.g. drug, disease, etc)
        example: DRUGBANK:DB00394
        required: true
        schema:
          type: string
```

2. Now, create the function in the [openpredict/openpredict_api.py](https://github.com/MaastrichtU-IDS/translator-openpredict/blob/master/openpredict/openpredict_api.py#L67) file

```python
def get_predict(entity='DB00001'):
    print("Do stuff with " + entity)
```

> The parameters provided in `openapi.yml` and the arguments of the function in `openpredict_api.py` need to match!

# Generate docs üìñ

Documentation in [docs/](docs/)  generated from the Python source code docstrings using [pydoc-markdown](https://pydoc-markdown.readthedocs.io/en/latest/).

```bash
pip3 install pydoc-markdown
```

Generate markdown documentation page for the `openpredict` package in `docs/`

```bash
pydoc-markdown --render-toc -p openpredict > docs/README.md
```

Modify the generated page title:

```bash
find docs/README.md -type f -exec sed -i "s/# Table of Contents/# OpenPredict Package documentation üîÆüêç/g" {} +
```

> This can also be done using Sphinx, see this article on [deploying Sphinx to GitHub Pages](https://circleci.com/blog/deploying-documentation-to-github-pages-with-continuous-integration/)
>
> ```bash
> pip3 install sphinx
> sphinx-quickstart sphinx-docs/ --project 'openpredict' --author 'Vincent Emonet'
> cd sphinx-docs/
> make html
> ```

# More about the data model

Metadata about runs, models evaluations, features are stored using the [ML Schema ontology](http://ml-schema.github.io/documentation/ML%20Schema.html) in a RDF triplestore (Ontotext GraphDB).

> See the [ML Schema documentation](http://ml-schema.github.io/documentation/ML%20Schema.html) for more details on the data model.

![OpenPredict datamodel](https://raw.githubusercontent.com/MaastrichtU-IDS/translator-openpredict/master/docs/OpenPREDICT_datamodel.jpg)

